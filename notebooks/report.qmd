---
title: "Orion Summer Internship (House Price Prediction Model)"
theme: solarized-dark
author: Kaustubh S Nair
date: last-modified
format:
  gfm:
    code-tools: true
    code-fold: true
    toc: true       
#jupyter: python3
---

# üéØThe AMES (Advanced Estate Management System)
 House Prediction Regression Model is a machine learning-based system designed to accurately predict house prices. The model leverages regression algorithms and advanced data analysis techniques to estimate the monetary value of residential properties based on a variety of input features. By analyzing historical data and learning from patterns, the AEMS model aims to provide reliable and precise predictions, facilitating informed decision-making in the real estate industry.The model incorporates various stages of data preprocessing and feature engineering to handle missing values, outliers, and transform variables into a suitable format for regression analysis. Techniques such as log transformation, one-hot encoding, and label encoding are applied to ensure the optimal representation of the data. The model also employs advanced regression algorithms, such as linear regression, decision trees, random forests, or gradient boosting, to capture complex relationships and accurately estimate house prices.

 

```{python}
import os
from dotenv import load_dotenv

load_dotenv()  # take environment variables from .env.
#ROOT_DIR1 =  os.environ.get("ROOT_DIR") 
#print(ROOT_DIR1)
ROOT_DIR =  os.environ.get("ROOT_DIR2") 

```


```{python}
import os
os.chdir(ROOT_DIR)
import numpy as np
import pandas as pd
import seaborn as sb
from sklearn.preprocessing import LabelEncoder
from src.util import catgvssale,contvssale,contvscont,check_column_skewness,remove_skewness,plot_contv,remove_ngskewness,log_transform_continuous,one_hot_encode_nominal,label_encode_dataset,preprocess_dataset
from datetime import datetime
df = pd.read_csv( "data/raw/train.csv")
temp_df=df
```

# üéØPRE-PROCESSING & EDA : 
EDA (Exploratory Data Analysis) and preprocessing play crucial roles in the development of the AEMS House Prediction Regression Model. These stages involve thorough analysis, cleaning, and transformation of the dataset to ensure optimal data quality and feature representation. Analyze the distribution of numerical features and address skewness or non-normality by applying transformations like log transformation.Convert categorical variables into numerical representations using one-hot encoding or label encoding techniques, depending on the nature and cardinality of the variables.

```{python}
numerics =['int16','int32','int64','float16','float32','float64']
numcol = df.select_dtypes(include=numerics)
len(numcol)
numcol
```

```{python}
missingper = df.isna().sum().sort_values(ascending=False)/len(df)
missingper.head(10)
miss_col = missingper[missingper!=0]
miss_col
miss_col.head(19)
```

## Columns with NO Missing_Values
```{python}
nomiss_col = missingper[missingper == 0]
nomiss_col
nomiss_col.info()
nomiss_col.head(15)
```

## Target:SalePrie
```{python}

#plot_contv(contvar="SalePrice",df=df)
remove_skewness(df,"SalePrice")
check_column_skewness(df,"SalePrice")
plot_contv(contvar="SalePrice",df=df)
```

## MSSubClass 
```{python}
catgvssale(catgvar="MSSubClass",df=df)
check_column_skewness(df,"MSSubClass")
```

## LotFrontage
Checking the skewness of a temporary df which does not include the values of the column LotFrontage which has value 0 and plotting it against the salePrice to check whether it is suitable for the model or not.

Skewness before log transform is 2.1635691423248837.

Now removing the skewness and checking the value againa and plotting it again to see the difference.

Skewness atfer log transform = -0.7287278423055492
```{python}

temp_df = df.loc[df['LotFrontage']!=0]
remove_skewness(temp_df,"LotFrontage")
check_column_skewness(temp_df,"LotFrontage")
contvssale(contvar="LotFrontage",df=temp_df)
```

## LotArea
Skewness = -0.1374044.

The value is acceptable as it has already been transformed using the log transformation.
```{python}
remove_skewness(temp_df,"LotArea")
contvssale(contvar="LotArea",df=temp_df)
```

## YearBuilt
Skewness = -0.6134611.

As the skewness of the column is already less than 1 there is no need to apply the log transformation.
```{python}
contvssale(contvar="YearBuilt",df=temp_df)
check_column_skewness(temp_df,"YearBuilt")
```

## YearRemodAdd
Skewness=-0.503562002.

As from the below countplot, boxplot and regplot we can see that this data is not skewed.
```{python}
contvssale(contvar="YearRemodAdd",df=df)
```

## YearRemodAdd vs YearBuilt
```{python}
contvscont(contvar="YearBuilt",df=df,tarvar="YearRemodAdd")
```

## MasVnrArea
Skewness = 2.677616.

From the below graph we can see that this data is a little positively skewed so we can apply here log transformation.

Skewness after = 0.50353171.
```{python}
op = remove_skewness(df,"MasVnrArea")
check_column_skewness(df,"MasVnrArea")
contvssale(contvar="MasVnrArea",df=df)
```

## BsmtFinSF1--
Skewness = 1.685503071910789.

From the regression plot as well as boxplot we can say that this data is slightly skewed as it has more confidence in the regression plot.

Skewness = -0.618409817855514.
```{python}
remove_skewness(df,"BsmtFinSF1")
check_column_skewness(df,"BsmtFinSF1")
temp_df = df.loc[df['BsmtFinSF1']!=0]
contvssale(contvar="BsmtFinSF1",df=temp_df)
check_column_skewness(temp_df,"BsmtFinSF1")

```

## BsmtFinSF2: Type 2 finished square feet
Skewness = 4.255261108933303.

The data is positively skewed and may impact our model so we apply log transform.
skewness = 2.434961825856814.
After removing the 0 values we get the column with skewness which is less than 1.
skewness = 0.9942372017307054
```{python}
remove_skewness(df,"BsmtFinSF2")
check_column_skewness(df,"BsmtFinSF2")
temp_df = df.loc[df['BsmtFinSF2']!=0]
contvssale(contvar="BsmtFinSF2",df=temp_df)
check_column_skewness(temp_df,"BsmtFinSF2")
```

## TotalBsmtSF: Total square feet of basement
skewness = 1.5242545490627664
```{python}
contvssale(contvar="TotalBsmtSF",df=df)
```

## GrLivArea: Above grade (ground) living area square feet
Skewness = 1.3665603560164552.

The skewness of this data is accepable so no need to apply the log transfrom as it would make it negatively skewed.
```{python}
contvssale(contvar="GrLivArea",df=df)
check_column_skewness(df,"GrLivArea")
```

## FullBath: Full bathrooms above grade
From the regression plot as well as the boxplot we can conclude that the data is not skewed.

It is a descrete variable as well.

Skewness = 0.036561558402727165
```{python}
check_column_skewness(df,"FullBath")
catgvssale(catgvar="FullBath",df=df)
```

## BedroomAbvGr: Bedrooms above grade 
It appears as a continuous column but according to its bar graph it is clear that it is descrete.
```{python}
catgvssale(catgvar="BedroomAbvGr",df=df)
```

## KitchenAbvGr: Kitchens above grade 
```{python}
catgvssale(catgvar="KitchenAbvGr",df=df)
```

## TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
From the below info we can see that the data is under the skew limit and the graph is normal.

Skewness = 0.6763408364355531
```{python}
check_column_skewness(df,"TotRmsAbvGrd")
catgvssale(catgvar="TotRmsAbvGrd",df=df)
```

## Fireplaces: Number of fireplaces
The data is under the skewness limit of 1.

It can be seen from the graphs as well.

skewness = 0.6495651830548841
```{python}
contvssale(contvar="Fireplaces",df=df)
check_column_skewness(df,"Fireplaces")
catgvssale(catgvar="Fireplaces",df=df)
```

## GarageCars: Size of garage in car capacity
From the below visualization it can be predicted that this data column is not skewed its normal.

skewness = -0.3425489297486655
```{python}
contvssale(contvar="GarageCars",df=df)
check_column_skewness(df,"GarageCars")
catgvssale(catgvar="GarageCars",df=df)
```

## GarageArea: Size of garage in square feet
skewness = 0.17998090674623907

Data column is acceptable.
```{python}
contvssale(contvar="GarageArea",df=df)
check_column_skewness(df,"GarageArea")
```

## garagecars vs garage area 
```{python}
contvscont(contvar="GarageArea",df=df,tarvar="GarageCars")
```

## OpenPorchSF: Open porch area in square feet(best)
skewness = 2.3643417403694404 

skewness after = -0.02339729485739231
```{python}
check_column_skewness(df,"OpenPorchSF")
remove_skewness(df,"OpenPorchSF")
temp_df=temp_df = df.loc[df['OpenPorchSF']!=0]
contvssale(contvar="OpenPorchSF",df=temp_df)
```

## ScreenPorch: Screen porch area in square feet
Skewness before transform = 4.122213743143115

Just after removing the 0 values the skewness came upto 1.186468489847003.

After removing the skewness values of the temp_df we get it -0.40 .
```{python}
check_column_skewness(df,"ScreenPorch")
temp_df=temp_df = df.loc[df['ScreenPorch']!=0]
contvssale(contvar="ScreenPorch",df=temp_df)
check_column_skewness(temp_df,"ScreenPorch")
```

## current age of the buildings according to the YearRemodAdd
```{python}
temp_df = df
current_year = datetime.now().year

temp_df['Age']= current_year-df['YearRemodAdd']
#print(temp_df)
temp_df['Age']
contvscont(contvar='Age',df=temp_df,tarvar='SalePrice')
```

## Age of the building acc to the yearsold
```{python}
temp_df['tempAge']= df['YrSold']-df['YearRemodAdd']
#print(temp_df)
temp_df['tempAge']
contvssale(contvar="tempAge",df=temp_df)
```

## garage year built
```{python}
temp_df = df
current_year = datetime.now().year

temp_df['GarAgeYr']= current_year-df['GarageYrBlt']
temp_df['GarAgeYr']
contvssale(contvar='GarAgeYr',df=temp_df)
```

# üéØTRAINING AND EVALUATION : 
Split the preprocessed dataset into training and testing sets to evaluate the model's performance on unseen data.Ensure proper stratification if the dataset is imbalanced or if specific classes or categories need to be represented equally in both sets.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.preprocessing import LabelEncoder
from src.util import catgvssale,contvssale,contvscont,check_column_skewness,remove_skewness,plot_contv,remove_ngskewness,preprocess_dataset
from datetime import datetime
from scipy.stats import skew
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectPercentile, chi2
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import pickle
```

```{python}
final_df = pd.read_csv('data/raw/train.csv')
df1 =  pd.read_csv('data/raw/train.csv')
lbl_dsc = pd.read_csv('data\clean\label_description.csv')
var_dsc =  pd.read_csv('data/clean/variable_description.csv')
```


## Select the variables that are Nominal from var_dsc
From the variable description file we are collecting only the variables that are nominal so that we can add it to the list of nominal data.
```{python}
nominal_df = var_dsc.loc[var_dsc['variable_type'] == 'Nominal']
#print(ordinal_df)
#nominal_df.head(30)
nominal_list = nominal_df['variable'].to_list()
print(nominal_list)
nominal_list.remove("MiscFeature")
nominal_list.remove("Alley")
nominal_list.remove("MSSubClass")
print(nominal_list)
```

## Select the variables that are Ordinal from var_dsc
From the variable description file we are collecting only the variables that are ordinal so that we can add it to the list of ordinal data.
```{python}
ordinal_df = var_dsc.loc[var_dsc['variable_type'] == 'Ordinal']
#print(ordinal_df)
ordinal_df.head(24)
ordinal_list = ordinal_df['variable'].to_list()
print(ordinal_list)
ordinal_list.remove("PoolQC")
ordinal_list.remove("Fence")
print(ordinal_list)
```

## Select the variables that are Continuous from var_dsc
From the variable description file we are collecting only the variables that are continuous so that we can add it to the list of continuous data.
```{python}
cont_df = var_dsc.loc[var_dsc['variable_type'] == 'Continuous']
cont_list = cont_df['variable'].to_list()
print(cont_list)
```

## Creating arguments for the function to be used
```{python}
ordinal_vars = ordinal_list
continuous_vars = cont_list
nominal_vars =  nominal_list
```

## Dropping the columns with the most missing ratio
```{python}
df1 = df1.drop(['Alley','PoolQC','MiscFeature','Fence'],axis=1)
df1.fillna(0)
print('Dropped columns are : Alley,PoolQC,MiscFeature,Fence')
```

## Applying labelencoding , one hot encoding and log tranfromation on the above generated lists
We apply labelencoding for the ordinal variables, one hot encoding for the nominal variables and log transformation for the continuous variables.
```{python}
def preprocess_dataset(dataset, continuous_vars, nominal_vars, ordinal_vars):
    print(dataset.shape)
    transformed_dataset = log_transform_continuous(dataset, continuous_vars)
    print(transformed_dataset.shape)
    encoded_dataset = one_hot_encode_nominal(transformed_dataset, nominal_vars)
    print(encoded_dataset.shape)
    preprocessed_dataset = label_encode_dataset(encoded_dataset, ordinal_vars)
    print(preprocessed_dataset.shape)
    return preprocessed_dataset
final_data = preprocess_dataset(df1, continuous_vars, nominal_vars, ordinal_vars)

```

## Splitting the data 
```{python}
columnindex = final_data.columns.get_loc('SalePrice')
print(columnindex)

X1 = final_data.iloc[:,0:columnindex]
y = final_data.iloc[:,columnindex]
X2 = final_data.iloc[:,(columnindex + 1):192]
X = pd.concat([X1,X2],axis=1)
X = X.fillna(0)
X.drop('Id',axis=1)
X
```

# MODEL 1 :
Linear Regression Model With the Mutual Information Data
```{python}
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SelectKBest, mutual_info_regression
from sklearn.model_selection import train_test_split
# Split the data into features (X) and target variable (y)

# Apply SelectKBest feature selection
k = 170 # Number of top features to select
selector = SelectKBest(score_func=mutual_info_regression, k=k)
X_selected = selector.fit_transform(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)


# Get selected feature names
#selected_feature_names = X.columns[selector.get_support()].tolist()

# Fit a linear regression model using the selected features
linear_regression = LinearRegression()
linear_regression.fit(X_train, y_train)

y_pred = linear_regression.predict(X_test)

y_predsp = np.expm1(y_pred)
y_testsp = np.expm1(y_test)

```

## Scores 
```{python}
mse = mean_squared_error(y_testsp, y_predsp)
r2 = r2_score(y_testsp, y_predsp)
rmse = mean_squared_error(y_testsp, y_predsp,squared=False)
print("Mean Squared Error (RMSE) on train set:", mse)
print("Root Mean Squared Error (MSE) on train set:", rmse)
print("R-squared score on train set:", r2)
le = round(linear_regression.score(X_test,y_test), 2)
print(le)
```

# MODEL 2 :
RandomForest Regression 
```{python}
from sklearn.ensemble import RandomForestRegressor
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)
# create regressor object
regressor = RandomForestRegressor(n_estimators=100,random_state=0,oob_score=True)
 
# fit the regressor with x and y data
regressor.fit(X_train, y_train)
Y_pred = regressor.predict(X_test)
Y_predsp = np.expm1(Y_pred)
Y_testsp = np.expm1(y_test)
```

## Scores of the RandomForest Regression
```{python}
print(regressor.oob_score_)
r2 = r2_score(Y_testsp, Y_predsp)
rmse = mean_squared_error(Y_testsp, Y_predsp,squared=False)
print("R-squared score on train set:", r2)
print("Root Mean Squared Error (MSE) on train set:", rmse)
```

# ‚û°Ô∏èCONCLUSION : 
From the above insights on the various scores and rmse and r2 values we can conclude that linear regression model with the mutual information data will be the most suitable for this house price prediction model.  